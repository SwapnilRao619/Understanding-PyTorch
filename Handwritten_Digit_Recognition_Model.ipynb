{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Handwritten Digit Recognition Model**"
      ],
      "metadata": {
        "id": "0O4fybHQKkLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') #device config\n",
        "\n",
        "inp_size=784 #hyper parameters\n",
        "hidd_size=500\n",
        "num_classes=10\n",
        "num_epochs=50\n",
        "batch_size=100\n",
        "lear_rate=0.001\n",
        "train_dataset=torchvision.datasets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True) #MNIST dataset\n",
        "test_dataset=torchvision.datasets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True) #shuffle true as avoids pattern memorization\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False) #while testing, want to check efficiency\n",
        "ex_data, ex_targets = next(iter(test_loader)) #batch input data and target data\n",
        "\n",
        "class NeuralNetwork(nn.Module): #design model\n",
        "  def __init__(self,input_size,hidden_size,num_classes):\n",
        "    super(NeuralNetwork,self).__init__()\n",
        "    self.l1=nn.Linear(input_size,hidden_size)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.l2=nn.Linear(hidden_size,num_classes)\n",
        "  def forward(self,x):\n",
        "    out=self.l1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.l2(out) #no activation and softmax at the end as crossentropyloss needs raw value\n",
        "    return out\n",
        "model=NeuralNetwork(inp_size,hidd_size,num_classes).to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss() #loss and optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=lear_rate)\n",
        "\n",
        "ntotsteps=len(train_loader) #training loop\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images,labels) in enumerate(train_loader):\n",
        "    images=images.reshape(-1,28*28).to(device)\n",
        "    labels=labels.to(device)\n",
        "    outputs=model(images)\n",
        "    loss=criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step() #update weights\n",
        "    optimizer.zero_grad()\n",
        "    if (i+1)%100==0:\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{ntotsteps}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNWzltdKS5Fj",
        "outputId": "f55fc05b-18b7-4e14-fed2-00502e982326"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [100/600], Loss: 0.35951682925224304\n",
            "Epoch [1/50], Step [200/600], Loss: 0.4348367750644684\n",
            "Epoch [1/50], Step [300/600], Loss: 0.25184258818626404\n",
            "Epoch [1/50], Step [400/600], Loss: 0.14750151336193085\n",
            "Epoch [1/50], Step [500/600], Loss: 0.09185905754566193\n",
            "Epoch [1/50], Step [600/600], Loss: 0.10715942084789276\n",
            "Epoch [2/50], Step [100/600], Loss: 0.08736006915569305\n",
            "Epoch [2/50], Step [200/600], Loss: 0.1479598581790924\n",
            "Epoch [2/50], Step [300/600], Loss: 0.11540376394987106\n",
            "Epoch [2/50], Step [400/600], Loss: 0.1534421294927597\n",
            "Epoch [2/50], Step [500/600], Loss: 0.1150631532073021\n",
            "Epoch [2/50], Step [600/600], Loss: 0.12144412100315094\n",
            "Epoch [3/50], Step [100/600], Loss: 0.09463109821081161\n",
            "Epoch [3/50], Step [200/600], Loss: 0.06898201256990433\n",
            "Epoch [3/50], Step [300/600], Loss: 0.10925927758216858\n",
            "Epoch [3/50], Step [400/600], Loss: 0.110145702958107\n",
            "Epoch [3/50], Step [500/600], Loss: 0.11700320243835449\n",
            "Epoch [3/50], Step [600/600], Loss: 0.0359172485768795\n",
            "Epoch [4/50], Step [100/600], Loss: 0.029657740145921707\n",
            "Epoch [4/50], Step [200/600], Loss: 0.030383724719285965\n",
            "Epoch [4/50], Step [300/600], Loss: 0.07708077877759933\n",
            "Epoch [4/50], Step [400/600], Loss: 0.048950500786304474\n",
            "Epoch [4/50], Step [500/600], Loss: 0.05216161161661148\n",
            "Epoch [4/50], Step [600/600], Loss: 0.07062972337007523\n",
            "Epoch [5/50], Step [100/600], Loss: 0.011100190691649914\n",
            "Epoch [5/50], Step [200/600], Loss: 0.04962354525923729\n",
            "Epoch [5/50], Step [300/600], Loss: 0.0036538103595376015\n",
            "Epoch [5/50], Step [400/600], Loss: 0.054218314588069916\n",
            "Epoch [5/50], Step [500/600], Loss: 0.0470125637948513\n",
            "Epoch [5/50], Step [600/600], Loss: 0.03952857479453087\n",
            "Epoch [6/50], Step [100/600], Loss: 0.010158861055970192\n",
            "Epoch [6/50], Step [200/600], Loss: 0.021667513996362686\n",
            "Epoch [6/50], Step [300/600], Loss: 0.06446761637926102\n",
            "Epoch [6/50], Step [400/600], Loss: 0.045896340161561966\n",
            "Epoch [6/50], Step [500/600], Loss: 0.03246649354696274\n",
            "Epoch [6/50], Step [600/600], Loss: 0.04490812122821808\n",
            "Epoch [7/50], Step [100/600], Loss: 0.029566537588834763\n",
            "Epoch [7/50], Step [200/600], Loss: 0.00903844740241766\n",
            "Epoch [7/50], Step [300/600], Loss: 0.011322774924337864\n",
            "Epoch [7/50], Step [400/600], Loss: 0.008833038620650768\n",
            "Epoch [7/50], Step [500/600], Loss: 0.010775970295071602\n",
            "Epoch [7/50], Step [600/600], Loss: 0.0185575932264328\n",
            "Epoch [8/50], Step [100/600], Loss: 0.02018772065639496\n",
            "Epoch [8/50], Step [200/600], Loss: 0.01581702195107937\n",
            "Epoch [8/50], Step [300/600], Loss: 0.024733958765864372\n",
            "Epoch [8/50], Step [400/600], Loss: 0.0196329765021801\n",
            "Epoch [8/50], Step [500/600], Loss: 0.014245924539864063\n",
            "Epoch [8/50], Step [600/600], Loss: 0.019214892759919167\n",
            "Epoch [9/50], Step [100/600], Loss: 0.024748370051383972\n",
            "Epoch [9/50], Step [200/600], Loss: 0.007435266859829426\n",
            "Epoch [9/50], Step [300/600], Loss: 0.015207196585834026\n",
            "Epoch [9/50], Step [400/600], Loss: 0.06039486825466156\n",
            "Epoch [9/50], Step [500/600], Loss: 0.006913584657013416\n",
            "Epoch [9/50], Step [600/600], Loss: 0.0332258976995945\n",
            "Epoch [10/50], Step [100/600], Loss: 0.02283765748143196\n",
            "Epoch [10/50], Step [200/600], Loss: 0.008992044255137444\n",
            "Epoch [10/50], Step [300/600], Loss: 0.014065506868064404\n",
            "Epoch [10/50], Step [400/600], Loss: 0.022638704627752304\n",
            "Epoch [10/50], Step [500/600], Loss: 0.029091713950037956\n",
            "Epoch [10/50], Step [600/600], Loss: 0.0018381725531071424\n",
            "Epoch [11/50], Step [100/600], Loss: 0.004254532512277365\n",
            "Epoch [11/50], Step [200/600], Loss: 0.006267066579312086\n",
            "Epoch [11/50], Step [300/600], Loss: 0.002245744690299034\n",
            "Epoch [11/50], Step [400/600], Loss: 0.004418192431330681\n",
            "Epoch [11/50], Step [500/600], Loss: 0.003621597308665514\n",
            "Epoch [11/50], Step [600/600], Loss: 0.0048271846026182175\n",
            "Epoch [12/50], Step [100/600], Loss: 0.00802313070744276\n",
            "Epoch [12/50], Step [200/600], Loss: 0.007966536097228527\n",
            "Epoch [12/50], Step [300/600], Loss: 0.013185283169150352\n",
            "Epoch [12/50], Step [400/600], Loss: 0.002261235611513257\n",
            "Epoch [12/50], Step [500/600], Loss: 0.0016759936697781086\n",
            "Epoch [12/50], Step [600/600], Loss: 0.0003161409986205399\n",
            "Epoch [13/50], Step [100/600], Loss: 0.02020188607275486\n",
            "Epoch [13/50], Step [200/600], Loss: 0.001153687248006463\n",
            "Epoch [13/50], Step [300/600], Loss: 0.0010926633840426803\n",
            "Epoch [13/50], Step [400/600], Loss: 0.005716128274798393\n",
            "Epoch [13/50], Step [500/600], Loss: 0.0024359822273254395\n",
            "Epoch [13/50], Step [600/600], Loss: 0.0064982157200574875\n",
            "Epoch [14/50], Step [100/600], Loss: 0.0013818771112710238\n",
            "Epoch [14/50], Step [200/600], Loss: 0.00477511715143919\n",
            "Epoch [14/50], Step [300/600], Loss: 0.003257732605561614\n",
            "Epoch [14/50], Step [400/600], Loss: 0.004887021612375975\n",
            "Epoch [14/50], Step [500/600], Loss: 0.03591929003596306\n",
            "Epoch [14/50], Step [600/600], Loss: 0.002470592502504587\n",
            "Epoch [15/50], Step [100/600], Loss: 0.0004594259080477059\n",
            "Epoch [15/50], Step [200/600], Loss: 0.008433466777205467\n",
            "Epoch [15/50], Step [300/600], Loss: 0.00552142271772027\n",
            "Epoch [15/50], Step [400/600], Loss: 0.0021579035092145205\n",
            "Epoch [15/50], Step [500/600], Loss: 0.0011517121456563473\n",
            "Epoch [15/50], Step [600/600], Loss: 0.0009246774134226143\n",
            "Epoch [16/50], Step [100/600], Loss: 0.001297625945881009\n",
            "Epoch [16/50], Step [200/600], Loss: 0.0027496113907545805\n",
            "Epoch [16/50], Step [300/600], Loss: 0.0005858016666024923\n",
            "Epoch [16/50], Step [400/600], Loss: 0.009704762138426304\n",
            "Epoch [16/50], Step [500/600], Loss: 0.014821801334619522\n",
            "Epoch [16/50], Step [600/600], Loss: 0.0013182786060497165\n",
            "Epoch [17/50], Step [100/600], Loss: 0.0008936719386838377\n",
            "Epoch [17/50], Step [200/600], Loss: 0.009666478261351585\n",
            "Epoch [17/50], Step [300/600], Loss: 0.005627134349197149\n",
            "Epoch [17/50], Step [400/600], Loss: 0.0023511110339313745\n",
            "Epoch [17/50], Step [500/600], Loss: 0.009424617514014244\n",
            "Epoch [17/50], Step [600/600], Loss: 0.0626230388879776\n",
            "Epoch [18/50], Step [100/600], Loss: 0.00013693486107513309\n",
            "Epoch [18/50], Step [200/600], Loss: 0.00047305479529313743\n",
            "Epoch [18/50], Step [300/600], Loss: 0.0065449601970613\n",
            "Epoch [18/50], Step [400/600], Loss: 0.0007126422133296728\n",
            "Epoch [18/50], Step [500/600], Loss: 0.0004800651513505727\n",
            "Epoch [18/50], Step [600/600], Loss: 0.019758310168981552\n",
            "Epoch [19/50], Step [100/600], Loss: 0.005502485204488039\n",
            "Epoch [19/50], Step [200/600], Loss: 0.0016385881463065743\n",
            "Epoch [19/50], Step [300/600], Loss: 0.0005380545044317842\n",
            "Epoch [19/50], Step [400/600], Loss: 0.0003332827182020992\n",
            "Epoch [19/50], Step [500/600], Loss: 0.0007662898278795183\n",
            "Epoch [19/50], Step [600/600], Loss: 0.00016663642600178719\n",
            "Epoch [20/50], Step [100/600], Loss: 0.00022943310614209622\n",
            "Epoch [20/50], Step [200/600], Loss: 0.0009269808069802821\n",
            "Epoch [20/50], Step [300/600], Loss: 0.01936125010251999\n",
            "Epoch [20/50], Step [400/600], Loss: 0.000259353662841022\n",
            "Epoch [20/50], Step [500/600], Loss: 0.015155800618231297\n",
            "Epoch [20/50], Step [600/600], Loss: 0.0022360188886523247\n",
            "Epoch [21/50], Step [100/600], Loss: 0.0034441109746694565\n",
            "Epoch [21/50], Step [200/600], Loss: 0.001998855732381344\n",
            "Epoch [21/50], Step [300/600], Loss: 0.004280437249690294\n",
            "Epoch [21/50], Step [400/600], Loss: 0.0012560939649119973\n",
            "Epoch [21/50], Step [500/600], Loss: 0.0011824831599369645\n",
            "Epoch [21/50], Step [600/600], Loss: 0.0003846580220852047\n",
            "Epoch [22/50], Step [100/600], Loss: 0.045022763311862946\n",
            "Epoch [22/50], Step [200/600], Loss: 0.00035248781205154955\n",
            "Epoch [22/50], Step [300/600], Loss: 0.00014998263213783503\n",
            "Epoch [22/50], Step [400/600], Loss: 0.0017103692516684532\n",
            "Epoch [22/50], Step [500/600], Loss: 0.0004545217379927635\n",
            "Epoch [22/50], Step [600/600], Loss: 0.0012062808964401484\n",
            "Epoch [23/50], Step [100/600], Loss: 0.003676351858302951\n",
            "Epoch [23/50], Step [200/600], Loss: 0.024512317031621933\n",
            "Epoch [23/50], Step [300/600], Loss: 0.00029002613155171275\n",
            "Epoch [23/50], Step [400/600], Loss: 0.0003747466835193336\n",
            "Epoch [23/50], Step [500/600], Loss: 0.0006071498501114547\n",
            "Epoch [23/50], Step [600/600], Loss: 0.0002995012328028679\n",
            "Epoch [24/50], Step [100/600], Loss: 3.397639375180006e-05\n",
            "Epoch [24/50], Step [200/600], Loss: 4.385172724141739e-05\n",
            "Epoch [24/50], Step [300/600], Loss: 0.000205426083994098\n",
            "Epoch [24/50], Step [400/600], Loss: 0.00030356121715158224\n",
            "Epoch [24/50], Step [500/600], Loss: 0.00010267695324728265\n",
            "Epoch [24/50], Step [600/600], Loss: 0.0002041593543253839\n",
            "Epoch [25/50], Step [100/600], Loss: 0.00014070152246858925\n",
            "Epoch [25/50], Step [200/600], Loss: 0.00016498944023624063\n",
            "Epoch [25/50], Step [300/600], Loss: 3.3123498724307865e-05\n",
            "Epoch [25/50], Step [400/600], Loss: 0.0001354889536742121\n",
            "Epoch [25/50], Step [500/600], Loss: 2.132141344191041e-05\n",
            "Epoch [25/50], Step [600/600], Loss: 0.00012329334276728332\n",
            "Epoch [26/50], Step [100/600], Loss: 1.2789111679012422e-05\n",
            "Epoch [26/50], Step [200/600], Loss: 0.00010249983461108059\n",
            "Epoch [26/50], Step [300/600], Loss: 0.00011824942339444533\n",
            "Epoch [26/50], Step [400/600], Loss: 5.1299313781782985e-05\n",
            "Epoch [26/50], Step [500/600], Loss: 8.466001600027084e-05\n",
            "Epoch [26/50], Step [600/600], Loss: 8.434579649474472e-05\n",
            "Epoch [27/50], Step [100/600], Loss: 8.007298310985789e-05\n",
            "Epoch [27/50], Step [200/600], Loss: 0.00010601262329146266\n",
            "Epoch [27/50], Step [300/600], Loss: 2.1569849195657298e-05\n",
            "Epoch [27/50], Step [400/600], Loss: 5.924964716541581e-05\n",
            "Epoch [27/50], Step [500/600], Loss: 0.0001131616736529395\n",
            "Epoch [27/50], Step [600/600], Loss: 0.00014920433750376105\n",
            "Epoch [28/50], Step [100/600], Loss: 4.390630783746019e-05\n",
            "Epoch [28/50], Step [200/600], Loss: 9.70138717093505e-05\n",
            "Epoch [28/50], Step [300/600], Loss: 2.596576996438671e-05\n",
            "Epoch [28/50], Step [400/600], Loss: 9.219698404194787e-05\n",
            "Epoch [28/50], Step [500/600], Loss: 4.337093196227215e-05\n",
            "Epoch [28/50], Step [600/600], Loss: 3.976113657699898e-05\n",
            "Epoch [29/50], Step [100/600], Loss: 6.283428956521675e-05\n",
            "Epoch [29/50], Step [200/600], Loss: 3.431682853261009e-05\n",
            "Epoch [29/50], Step [300/600], Loss: 1.9593979232013226e-05\n",
            "Epoch [29/50], Step [400/600], Loss: 1.8532951798988506e-05\n",
            "Epoch [29/50], Step [500/600], Loss: 0.0001961428497452289\n",
            "Epoch [29/50], Step [600/600], Loss: 3.7430245356517844e-06\n",
            "Epoch [30/50], Step [100/600], Loss: 5.341341602616012e-05\n",
            "Epoch [30/50], Step [200/600], Loss: 4.287499177735299e-05\n",
            "Epoch [30/50], Step [300/600], Loss: 1.7491536709712818e-05\n",
            "Epoch [30/50], Step [400/600], Loss: 7.659977563889697e-05\n",
            "Epoch [30/50], Step [500/600], Loss: 3.4129436244256794e-05\n",
            "Epoch [30/50], Step [600/600], Loss: 1.3674987712875009e-05\n",
            "Epoch [31/50], Step [100/600], Loss: 2.6725141651695594e-05\n",
            "Epoch [31/50], Step [200/600], Loss: 1.8901606381405145e-05\n",
            "Epoch [31/50], Step [300/600], Loss: 0.0821981355547905\n",
            "Epoch [31/50], Step [400/600], Loss: 0.02036936767399311\n",
            "Epoch [31/50], Step [500/600], Loss: 0.002113142516463995\n",
            "Epoch [31/50], Step [600/600], Loss: 0.0037432569079101086\n",
            "Epoch [32/50], Step [100/600], Loss: 0.014803090132772923\n",
            "Epoch [32/50], Step [200/600], Loss: 0.0010974765755236149\n",
            "Epoch [32/50], Step [300/600], Loss: 0.00010038351319963112\n",
            "Epoch [32/50], Step [400/600], Loss: 0.004578319378197193\n",
            "Epoch [32/50], Step [500/600], Loss: 0.004274859558790922\n",
            "Epoch [32/50], Step [600/600], Loss: 0.04183345288038254\n",
            "Epoch [33/50], Step [100/600], Loss: 0.00019528399570845068\n",
            "Epoch [33/50], Step [200/600], Loss: 0.0004887011600658298\n",
            "Epoch [33/50], Step [300/600], Loss: 1.4381326764123514e-05\n",
            "Epoch [33/50], Step [400/600], Loss: 0.0003724698326550424\n",
            "Epoch [33/50], Step [500/600], Loss: 0.00027353776386007667\n",
            "Epoch [33/50], Step [600/600], Loss: 0.0015437341062352061\n",
            "Epoch [34/50], Step [100/600], Loss: 0.00017891170864459127\n",
            "Epoch [34/50], Step [200/600], Loss: 0.00023705791682004929\n",
            "Epoch [34/50], Step [300/600], Loss: 0.0002119585988111794\n",
            "Epoch [34/50], Step [400/600], Loss: 0.002415227936580777\n",
            "Epoch [34/50], Step [500/600], Loss: 0.0002878532977774739\n",
            "Epoch [34/50], Step [600/600], Loss: 2.4485154426656663e-05\n",
            "Epoch [35/50], Step [100/600], Loss: 7.116940105333924e-05\n",
            "Epoch [35/50], Step [200/600], Loss: 0.0008397828787565231\n",
            "Epoch [35/50], Step [300/600], Loss: 5.038482777308673e-05\n",
            "Epoch [35/50], Step [400/600], Loss: 0.000156437570694834\n",
            "Epoch [35/50], Step [500/600], Loss: 7.851190275687259e-06\n",
            "Epoch [35/50], Step [600/600], Loss: 6.46531188976951e-05\n",
            "Epoch [36/50], Step [100/600], Loss: 6.638719059992582e-05\n",
            "Epoch [36/50], Step [200/600], Loss: 6.278639193624258e-05\n",
            "Epoch [36/50], Step [300/600], Loss: 8.063693530857563e-05\n",
            "Epoch [36/50], Step [400/600], Loss: 0.00013527461851481348\n",
            "Epoch [36/50], Step [500/600], Loss: 3.992233905592002e-05\n",
            "Epoch [36/50], Step [600/600], Loss: 0.00024306819250341505\n",
            "Epoch [37/50], Step [100/600], Loss: 3.5136748920194805e-05\n",
            "Epoch [37/50], Step [200/600], Loss: 5.69469848414883e-05\n",
            "Epoch [37/50], Step [300/600], Loss: 5.342532313079573e-05\n",
            "Epoch [37/50], Step [400/600], Loss: 4.343640466686338e-05\n",
            "Epoch [37/50], Step [500/600], Loss: 0.00011887087748618796\n",
            "Epoch [37/50], Step [600/600], Loss: 7.882553290983196e-06\n",
            "Epoch [38/50], Step [100/600], Loss: 7.767570059513673e-05\n",
            "Epoch [38/50], Step [200/600], Loss: 1.9221264665247872e-05\n",
            "Epoch [38/50], Step [300/600], Loss: 1.795758907974232e-05\n",
            "Epoch [38/50], Step [400/600], Loss: 8.780635107541457e-05\n",
            "Epoch [38/50], Step [500/600], Loss: 6.508849037345499e-05\n",
            "Epoch [38/50], Step [600/600], Loss: 0.0001038036571117118\n",
            "Epoch [39/50], Step [100/600], Loss: 6.382422725437209e-05\n",
            "Epoch [39/50], Step [200/600], Loss: 1.9153674656990916e-05\n",
            "Epoch [39/50], Step [300/600], Loss: 8.867179712979123e-05\n",
            "Epoch [39/50], Step [400/600], Loss: 3.466747875791043e-05\n",
            "Epoch [39/50], Step [500/600], Loss: 2.8214850317453966e-05\n",
            "Epoch [39/50], Step [600/600], Loss: 1.927708217408508e-05\n",
            "Epoch [40/50], Step [100/600], Loss: 5.1307280955370516e-05\n",
            "Epoch [40/50], Step [200/600], Loss: 1.3257767932373099e-05\n",
            "Epoch [40/50], Step [300/600], Loss: 3.802584615186788e-05\n",
            "Epoch [40/50], Step [400/600], Loss: 1.821126716095023e-05\n",
            "Epoch [40/50], Step [500/600], Loss: 7.840000034775585e-05\n",
            "Epoch [40/50], Step [600/600], Loss: 2.3779190087225288e-05\n",
            "Epoch [41/50], Step [100/600], Loss: 9.452471203985624e-06\n",
            "Epoch [41/50], Step [200/600], Loss: 9.808596587390639e-06\n",
            "Epoch [41/50], Step [300/600], Loss: 3.557290983735584e-05\n",
            "Epoch [41/50], Step [400/600], Loss: 2.464557474013418e-05\n",
            "Epoch [41/50], Step [500/600], Loss: 9.71780264080735e-06\n",
            "Epoch [41/50], Step [600/600], Loss: 3.165389716741629e-05\n",
            "Epoch [42/50], Step [100/600], Loss: 3.926901990780607e-05\n",
            "Epoch [42/50], Step [200/600], Loss: 2.98246823149384e-06\n",
            "Epoch [42/50], Step [300/600], Loss: 4.961660670232959e-05\n",
            "Epoch [42/50], Step [400/600], Loss: 6.783274875488132e-05\n",
            "Epoch [42/50], Step [500/600], Loss: 2.92248951154761e-05\n",
            "Epoch [42/50], Step [600/600], Loss: 1.3070120076008607e-05\n",
            "Epoch [43/50], Step [100/600], Loss: 2.664235353222466e-06\n",
            "Epoch [43/50], Step [200/600], Loss: 5.779923412774224e-06\n",
            "Epoch [43/50], Step [300/600], Loss: 2.2339423594530672e-05\n",
            "Epoch [43/50], Step [400/600], Loss: 1.4709152310388163e-05\n",
            "Epoch [43/50], Step [500/600], Loss: 5.46487899555359e-06\n",
            "Epoch [43/50], Step [600/600], Loss: 2.140778087778017e-05\n",
            "Epoch [44/50], Step [100/600], Loss: 1.7656175259617157e-05\n",
            "Epoch [44/50], Step [200/600], Loss: 8.951220479502808e-06\n",
            "Epoch [44/50], Step [300/600], Loss: 5.654745109495707e-06\n",
            "Epoch [44/50], Step [400/600], Loss: 7.574082246719627e-06\n",
            "Epoch [44/50], Step [500/600], Loss: 4.0864001675799955e-06\n",
            "Epoch [44/50], Step [600/600], Loss: 1.0152220056625083e-05\n",
            "Epoch [45/50], Step [100/600], Loss: 7.564633961010259e-06\n",
            "Epoch [45/50], Step [200/600], Loss: 7.380379429378081e-06\n",
            "Epoch [45/50], Step [300/600], Loss: 4.564203482004814e-05\n",
            "Epoch [45/50], Step [400/600], Loss: 5.594157755695051e-06\n",
            "Epoch [45/50], Step [500/600], Loss: 1.7698162992019206e-05\n",
            "Epoch [45/50], Step [600/600], Loss: 1.4060094144952018e-05\n",
            "Epoch [46/50], Step [100/600], Loss: 1.1587601875362452e-05\n",
            "Epoch [46/50], Step [200/600], Loss: 1.050223431775521e-06\n",
            "Epoch [46/50], Step [300/600], Loss: 2.722565341173322e-06\n",
            "Epoch [46/50], Step [400/600], Loss: 9.748985576152336e-06\n",
            "Epoch [46/50], Step [500/600], Loss: 1.763043428582023e-06\n",
            "Epoch [46/50], Step [600/600], Loss: 0.00027076801052317023\n",
            "Epoch [47/50], Step [100/600], Loss: 0.06007726490497589\n",
            "Epoch [47/50], Step [200/600], Loss: 0.025204606354236603\n",
            "Epoch [47/50], Step [300/600], Loss: 0.024265440180897713\n",
            "Epoch [47/50], Step [400/600], Loss: 0.0008943557040765882\n",
            "Epoch [47/50], Step [500/600], Loss: 0.003321999916806817\n",
            "Epoch [47/50], Step [600/600], Loss: 0.0001567528524901718\n",
            "Epoch [48/50], Step [100/600], Loss: 9.189108095597476e-05\n",
            "Epoch [48/50], Step [200/600], Loss: 0.0008878363878466189\n",
            "Epoch [48/50], Step [300/600], Loss: 9.588558168616146e-05\n",
            "Epoch [48/50], Step [400/600], Loss: 6.048072100384161e-05\n",
            "Epoch [48/50], Step [500/600], Loss: 2.8656518225034233e-06\n",
            "Epoch [48/50], Step [600/600], Loss: 5.746137321693823e-05\n",
            "Epoch [49/50], Step [100/600], Loss: 0.00030274622258730233\n",
            "Epoch [49/50], Step [200/600], Loss: 0.0003704447008203715\n",
            "Epoch [49/50], Step [300/600], Loss: 5.769963900092989e-05\n",
            "Epoch [49/50], Step [400/600], Loss: 2.9973709388286807e-05\n",
            "Epoch [49/50], Step [500/600], Loss: 9.463897731620818e-05\n",
            "Epoch [49/50], Step [600/600], Loss: 0.0010501283686608076\n",
            "Epoch [50/50], Step [100/600], Loss: 0.0001873396395239979\n",
            "Epoch [50/50], Step [200/600], Loss: 3.826257670880295e-05\n",
            "Epoch [50/50], Step [300/600], Loss: 1.408013758918969e-05\n",
            "Epoch [50/50], Step [400/600], Loss: 4.994755727238953e-05\n",
            "Epoch [50/50], Step [500/600], Loss: 5.139773293194594e-06\n",
            "Epoch [50/50], Step [600/600], Loss: 0.00021919154096394777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digit=int(input(\"Enter a number: \"))\n",
        "model.eval() #model in eval mode\n",
        "images,labels=test_dataset[digit]\n",
        "images=images.view(1,-1).to(device) #flatten input image\n",
        "output=model(images)\n",
        "predicted=output.argmax(dim=1,keepdim=True).item()\n",
        "print(f\"Prediction: {predicted}\")\n",
        "img=images.view(28,28).cpu().numpy() #flattened image back to original shape\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "SVhNPwPfWvKv",
        "outputId": "017efdfd-d833-41e6-ff9d-72ce438ef201"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a number: 9\n",
            "Prediction: 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}